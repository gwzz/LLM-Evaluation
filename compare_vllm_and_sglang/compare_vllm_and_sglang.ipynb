{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e2e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "class ModelBenchmark:\n",
    "    def __init__(self, model_configs: Dict[str, Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Initialize model benchmark class\n",
    "        \n",
    "        Args:\n",
    "            model_configs: Model configuration dictionary, format as {\n",
    "                \"model_name_1\": {\"api_base\": \"http://localhost:8001\", \"api_key\": \"empty\"},\n",
    "                \"model_name_2\": {\"api_base\": \"http://localhost:8002\", \"api_key\": \"empty\"}\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        for model_name, config in model_configs.items():\n",
    "            self.models[model_name] = OpenAILike(\n",
    "                model=model_name,\n",
    "                api_base=config[\"api_base\"],\n",
    "                api_key=config.get(\"api_key\", \"sk-xxx\"),\n",
    "                is_chat_model=True,\n",
    "                max_retries=3,\n",
    "                timeout=100,\n",
    "                additional_kwargs={\"extra_body\": {\"chat_template_kwargs\": {\"enable_thinking\": False}}}\n",
    "            )\n",
    "    \n",
    "    def create_test_messages(self, prompt: str) -> List[ChatMessage]:\n",
    "        \"\"\"Create test conversation messages\"\"\"\n",
    "        return [\n",
    "            ChatMessage(role=\"system\", content=\"You are a helpful AI assistant.\"),\n",
    "            ChatMessage(role=\"user\", content=prompt)\n",
    "        ]\n",
    "    \n",
    "    async def test_single_model(self, model_name: str, prompt: str, num_runs: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Test single model performance\"\"\"\n",
    "        print(f\"Testing model: {model_name}\")\n",
    "        \n",
    "        results = {\n",
    "            \"model\": model_name,\n",
    "            \"total_time\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"response_times\": [],\n",
    "            \"throughputs\": [],\n",
    "            \"success_count\": 0\n",
    "        }\n",
    "        \n",
    "        messages = self.create_test_messages(prompt)\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = await self.models[model_name].acomplete(\n",
    "                    prompt,\n",
    "                    messages\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                \n",
    "                # Calculate token count (approximate value)\n",
    "                tokens = len(response.text.split())\n",
    "                \n",
    "                results[\"total_time\"] += elapsed_time\n",
    "                results[\"total_tokens\"] += tokens\n",
    "                results[\"response_times\"].append(elapsed_time)\n",
    "                results[\"throughputs\"].append(tokens / elapsed_time)\n",
    "                results[\"success_count\"] += 1\n",
    "                \n",
    "                print(f\"  Run {i+1}/{num_runs}: {elapsed_time:.2f}s, {tokens} tokens\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Run {i+1}/{num_runs} failed: {str(e)}\")\n",
    "            \n",
    "            # Add short delay between runs\n",
    "            await asyncio.sleep(1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def compare_models(self, prompts: List[str], num_runs: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Compare performance of multiple models\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            print(f\"\\nTest prompt: '{prompt[:50]}...'\")\n",
    "            \n",
    "            prompt_results = {\"prompt\": prompt}\n",
    "            \n",
    "            # Run tests for each model\n",
    "            tasks = []\n",
    "            for model_name in self.models.keys():\n",
    "                task = self.test_single_model(model_name, prompt, num_runs)\n",
    "                tasks.append(task)\n",
    "            \n",
    "            model_results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # Process results\n",
    "            for result in model_results:\n",
    "                model_name = result[\"model\"]\n",
    "                if result[\"success_count\"] > 0:\n",
    "                    avg_time = result[\"total_time\"] / result[\"success_count\"]\n",
    "                    avg_throughput = result[\"total_tokens\"] / result[\"total_time\"] if result[\"total_time\"] > 0 else 0\n",
    "                    \n",
    "                    prompt_results[f\"{model_name}_avg_time\"] = avg_time\n",
    "                    prompt_results[f\"{model_name}_avg_throughput\"] = avg_throughput\n",
    "                    prompt_results[f\"{model_name}_success_rate\"] = result[\"success_count\"] / num_runs\n",
    "                else:\n",
    "                    prompt_results[f\"{model_name}_avg_time\"] = None\n",
    "                    prompt_results[f\"{model_name}_avg_throughput\"] = None\n",
    "                    prompt_results[f\"{model_name}_success_rate\"] = 0\n",
    "            \n",
    "            all_results.append(prompt_results)\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def generate_report(self, results_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate performance comparison report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"Large Language Model Inference Speed Comparison Report\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        for index, row in results_df.iterrows():\n",
    "            report.append(f\"\\nPrompt {index + 1}: '{row['prompt'][:50]}...'\")\n",
    "            report.append(\"-\" * 40)\n",
    "            \n",
    "            for model_name in self.models.keys():\n",
    "                if f\"{model_name}_avg_time\" in row:\n",
    "                    report.append(\n",
    "                        f\"{model_name}: \"\n",
    "                        f\"{row[f'{model_name}_avg_time']:.2f}s, \"\n",
    "                        f\"{row[f'{model_name}_avg_throughput']:.1f} tokens/s, \"\n",
    "                        f\"Success rate: {row[f'{model_name}_success_rate']:.0%}\"\n",
    "                    )\n",
    "        \n",
    "        # Calculate overall averages\n",
    "        report.append(\"\\n\" + \"=\" * 60)\n",
    "        report.append(\"Overall Performance Comparison\")\n",
    "        report.append(\"=\" * 60)\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            time_col = f\"{model_name}_avg_time\"\n",
    "            throughput_col = f\"{model_name}_avg_throughput\"\n",
    "            \n",
    "            if time_col in results_df.columns:\n",
    "                avg_time = results_df[time_col].mean()\n",
    "                avg_throughput = results_df[throughput_col].mean()\n",
    "                report.append(f\"{model_name}: Average response time {avg_time:.2f}s, Average throughput {avg_throughput:.1f} tokens/s\")\n",
    "        \n",
    "        return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your models\n",
    "model_configs = {\n",
    "    \"qwen3\": {\n",
    "        \"api_base\": \"http://192.168.100.30:16001/v1\",\n",
    "        \"api_key\": \"empty\"\n",
    "    },\n",
    "    \"qwen3-vllm\": {\n",
    "        \"api_base\": \"http://192.168.100.30:16002/v1\",\n",
    "        \"api_key\": \"empty\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Please explain the basic concepts of machine learning\",\n",
    "    \"Write a short story about artificial intelligence\",\n",
    "    \"How to improve programming skills? Please provide specific suggestions\",\n",
    "    \"Write a quicksort algorithm in Python\",\n",
    "    \"Discuss the impact of climate change on the global economy\"\n",
    "]\n",
    "\n",
    "# Create benchmark instance\n",
    "benchmark = ModelBenchmark(model_configs)\n",
    "\n",
    "# Run tests\n",
    "print(\"Starting model performance comparison test...\")\n",
    "results = await benchmark.compare_models(test_prompts, num_runs=3)\n",
    "\n",
    "# Generate report\n",
    "report = benchmark.generate_report(results)\n",
    "print(report)\n",
    "\n",
    "# Save results to CSV\n",
    "results.to_csv(\"model_benchmark_results.csv\", index=False)\n",
    "print(\"\\nDetailed results saved to model_benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('model_benchmark_results.csv')\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Qwen3 Model Benchmark Comparison: sglang vs vLLM', fontsize=16)\n",
    "\n",
    "# Prepare data for plotting\n",
    "models = ['sglang', 'vLLM']\n",
    "x_pos = np.arange(len(df))\n",
    "\n",
    "# 1. Plot Average Time Comparison\n",
    "ax1 = axes[0, 0]\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x_pos - width/2, df['qwen3_avg_time'], width, label='sglang', color='skyblue')\n",
    "bars2 = ax1.bar(x_pos + width/2, df['qwen3-vllm_avg_time'], width, label='vLLM', color='lightcoral')\n",
    "ax1.set_xlabel('Prompts')\n",
    "ax1.set_ylabel('Average Time (seconds)')\n",
    "ax1.set_title('Average Response Time Comparison')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'Prompt {i+1}' for i in range(len(df))], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.1f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.1f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Plot Throughput Comparison\n",
    "ax2 = axes[0, 1]\n",
    "bars1 = ax2.bar(x_pos - width/2, df['qwen3_avg_throughput'], width, label='sglang', color='lightgreen')\n",
    "bars2 = ax2.bar(x_pos + width/2, df['qwen3-vllm_avg_throughput'], width, label='vLLM', color='orange')\n",
    "ax2.set_xlabel('Prompts')\n",
    "ax2.set_ylabel('Throughput (tokens/second)')\n",
    "ax2.set_title('Average Throughput Comparison')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'Prompt {i+1}' for i in range(len(df))], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Plot Success Rate\n",
    "ax3 = axes[1, 0]\n",
    "success_sglang = df['qwen3_success_rate']\n",
    "success_vllm = df['qwen3-vllm_success_rate']\n",
    "bars1 = ax3.bar(x_pos - width/2, success_sglang, width, label='sglang', color='gold')\n",
    "bars2 = ax3.bar(x_pos + width/2, success_vllm, width, label='vLLM', color='mediumpurple')\n",
    "ax3.set_xlabel('Prompts')\n",
    "ax3.set_ylabel('Success Rate')\n",
    "ax3.set_title('Success Rate Comparison')\n",
    "ax3.set_ylim([0, 1.1])\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'Prompt {i+1}' for i in range(len(df))], rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Performance Summary Radar Chart\n",
    "ax4 = axes[1, 1]\n",
    "# Calculate average metrics for each model\n",
    "avg_time_sglang = df['qwen3_avg_time'].mean()\n",
    "avg_time_vllm = df['qwen3-vllm_avg_time'].mean()\n",
    "avg_throughput_sglang = df['qwen3_avg_throughput'].mean()\n",
    "avg_throughput_vllm = df['qwen3-vllm_avg_throughput'].mean()\n",
    "\n",
    "# Normalize data for radar chart (higher is better)\n",
    "# For time, we invert the values since lower time is better\n",
    "max_time = max(avg_time_sglang, avg_time_vllm)\n",
    "norm_time_sglang = (max_time - avg_time_sglang) / max_time\n",
    "norm_time_vllm = (max_time - avg_time_vllm) / max_time\n",
    "\n",
    "# For throughput, higher is better\n",
    "max_throughput = max(avg_throughput_sglang, avg_throughput_vllm)\n",
    "norm_throughput_sglang = avg_throughput_sglang / max_throughput\n",
    "norm_throughput_vllm = avg_throughput_vllm / max_throughput\n",
    "\n",
    "# Success rate is already normalized\n",
    "avg_success_sglang = df['qwen3_success_rate'].mean()\n",
    "avg_success_vllm = df['qwen3-vllm_success_rate'].mean()\n",
    "\n",
    "# Radar chart data\n",
    "categories = ['Response Time', 'Throughput', 'Success Rate']\n",
    "sglang_values = [norm_time_sglang, norm_throughput_sglang, avg_success_sglang]\n",
    "vllm_values = [norm_time_vllm, norm_throughput_vllm, avg_success_vllm]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "sglang_values += sglang_values[:1]  # Close the loop\n",
    "vllm_values += vllm_values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "ax4.plot(angles, sglang_values, 'o-', linewidth=2, label='sglang', color='blue')\n",
    "ax4.fill(angles, sglang_values, alpha=0.25, color='blue')\n",
    "ax4.plot(angles, vllm_values, 'o-', linewidth=2, label='vLLM', color='red')\n",
    "ax4.fill(angles, vllm_values, alpha=0.25, color='red')\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(categories)\n",
    "ax4.set_title('Performance Summary')\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"==================\")\n",
    "print(f\"Average response time (sglang): {avg_time_sglang:.2f} seconds\")\n",
    "print(f\"Average response time (vLLM): {avg_time_vllm:.2f} seconds\")\n",
    "print(f\"Average throughput (sglang): {avg_throughput_sglang:.2f} tokens/second\")\n",
    "print(f\"Average throughput (vLLM): {avg_throughput_vllm:.2f} tokens/second\")\n",
    "print(f\"Average success rate (sglang): {avg_success_sglang:.2f}\")\n",
    "print(f\"Average success rate (vLLM): {avg_success_vllm:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
